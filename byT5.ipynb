{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate sacrebleu"
      ],
      "metadata": {
        "id": "Mfxdqv810FvL"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8MvY-mPWENS"
      },
      "outputs": [],
      "source": [
        "!wget https://pmb.let.rug.nl/releases/pmb-4.0.0.zip\n",
        "!unzip -q \"pmb-4.0.0.zip\" \"*/gold/*\" -d .\n",
        "!unzip -q \"pmb-4.0.0.zip\" \"*/silver/*\" -d ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WPoelman/ud-boxer.git ud_boxer_repo\n",
        "!pip install -r ud_boxer_repo/requirements/requirements.txt"
      ],
      "metadata": {
        "id": "c9Sq29nY995F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py"
      ],
      "metadata": {
        "id": "f--Ro0Ni1f-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/ud_boxer_repo')"
      ],
      "metadata": {
        "id": "H-zIGiGOAVaN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, \n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "\n",
        "from ud_boxer_repo.ud_boxer.sbn import SBNGraph\n",
        "from ud_boxer_repo.ud_boxer.helpers import smatch_score"
      ],
      "metadata": {
        "id": "unQm4iDv4qCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "_ZcYagW74y5X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "space_re = re.compile('\\s+')\n",
        "\n",
        "data_dir = os.path.join('pmb-4.0.0', 'data')\n",
        "data = []\n",
        "\n",
        "for lang in os.listdir(data_dir):\n",
        "  for quality in ['gold', 'silver']:\n",
        "    quality_folder = os.path.join(data_dir, lang, quality)\n",
        "\n",
        "    for upper_filedir in os.listdir(quality_folder):\n",
        "      filedirs = os.path.join(quality_folder, upper_filedir)\n",
        "\n",
        "      for filedir in os.listdir(filedirs):\n",
        "        file_folder = os.path.join(filedirs, filedir)\n",
        "\n",
        "        with open(os.path.join(file_folder, f'{lang}.status')) as f:\n",
        "          if 'bronze' in f.read().lower():\n",
        "            continue\n",
        "\n",
        "        with open(os.path.join(file_folder, f'{lang}.raw')) as f:\n",
        "          raw_text = f.read().strip()\n",
        "        \n",
        "        with open(os.path.join(file_folder, f'{lang}.drs.sbn')) as f:\n",
        "          raw_drs = f.read().strip()\n",
        "          drs = '\\n'.join(\n",
        "              space_re.sub(' ', line.split('%')[0]).strip()\n",
        "              for line in raw_drs.split('\\n') \n",
        "              if not line.startswith('%%%')\n",
        "          )\n",
        "\n",
        "        data.append({\n",
        "            'lang': lang, \n",
        "            'quality': quality, \n",
        "            'text': raw_text, \n",
        "            'drs': drs\n",
        "        })"
      ],
      "metadata": {
        "id": "MeWig_vJvh68"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = defaultdict(lambda: defaultdict(int))\n",
        "for info in data:\n",
        "  stats[info['lang']][info['quality']] += 1\n",
        "\n",
        "for l, qualities in stats.items():\n",
        "  for q, qnum in qualities.items():\n",
        "    print(l, q, qnum)"
      ],
      "metadata": {
        "id": "cTvA9VOmx37D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e6c86c-1941-4f1a-b696-aea71a613bd9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nl gold 1467\n",
            "nl silver 3\n",
            "it gold 1686\n",
            "it silver 9\n",
            "de gold 2844\n",
            "de silver 16\n",
            "en gold 10715\n",
            "en silver 428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy881YXgGTqT",
        "outputId": "cf2f507d-6c47-42e3-db92-d864d2e75ad8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lang': 'nl',\n",
              " 'quality': 'gold',\n",
              " 'text': 'Lukoil verdiende in 2004 een miljard dollar.',\n",
              " 'drs': 'company.n.01 Name \"Lukoil\"\\nearn.v.01 Agent -1 Time +1 Theme +2\\ntime.n.08 YearOfCentury 2004 TPR now\\nmeasure.n.02 Quantity 1000000000 Unit +1\\ndollar.n.01'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_list(data)\n",
        "ds"
      ],
      "metadata": {
        "id": "LU94xdOq1zO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dca049a-aa4d-459a-81c7-814c8a73b7da"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['lang', 'quality', 'text', 'drs'],\n",
              "    num_rows: 17168\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")"
      ],
      "metadata": {
        "id": "wqCq4ulGzrhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 512\n",
        "\n",
        "def process(examples):\n",
        "  model_inputs = tokenizer(examples['text'], max_length=max_len, truncation=True)\n",
        "  labels = tokenizer(examples['drs'], max_length=max_len, truncation=True)\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "ds = ds.map(process, batched=True)"
      ],
      "metadata": {
        "id": "kQoEQXHA0Rpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.train_test_split(test_size=0.1, seed=SEED)"
      ],
      "metadata": {
        "id": "ifSowgTy4rut"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"chrf\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"chrf\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kHLB1c2R-hkh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pad_token_id = -100\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "3Uc5sMB2oIeS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='results',\n",
        "    report_to='none',\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=3,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=max_len,\n",
        "    generation_num_beams=3,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=ds['train'],\n",
        "  eval_dataset=ds['test'],\n",
        "  tokenizer=tokenizer,\n",
        "  data_collator=data_collator,\n",
        "  compute_metrics=compute_metrics\n",
        "  # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        ")"
      ],
      "metadata": {
        "id": "ofI3Fwqn0Ql7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cd9a2c-f460-43fb-f7cb-0cb350a39044"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "efX8RZzu5KBq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39107d9a-310a-4d7b-d194-e6e9058870f4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: lang, text, drs, quality. If lang, text, drs, quality are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 15451\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 11589\n",
            "  Number of trainable parameters = 299637760\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11589' max='11589' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11589/11589 1:42:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Chrf</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.256900</td>\n",
              "      <td>0.173386</td>\n",
              "      <td>68.203400</td>\n",
              "      <td>111.726300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.190400</td>\n",
              "      <td>0.132402</td>\n",
              "      <td>76.569000</td>\n",
              "      <td>110.135100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.166500</td>\n",
              "      <td>0.122962</td>\n",
              "      <td>78.631700</td>\n",
              "      <td>108.185800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: lang, text, drs, quality. If lang, text, drs, quality are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1717\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to results/checkpoint-3863\n",
            "Configuration saved in results/checkpoint-3863/config.json\n",
            "Model weights saved in results/checkpoint-3863/pytorch_model.bin\n",
            "tokenizer config file saved in results/checkpoint-3863/tokenizer_config.json\n",
            "Special tokens file saved in results/checkpoint-3863/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: lang, text, drs, quality. If lang, text, drs, quality are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1717\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to results/checkpoint-7726\n",
            "Configuration saved in results/checkpoint-7726/config.json\n",
            "Model weights saved in results/checkpoint-7726/pytorch_model.bin\n",
            "tokenizer config file saved in results/checkpoint-7726/tokenizer_config.json\n",
            "Special tokens file saved in results/checkpoint-7726/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: lang, text, drs, quality. If lang, text, drs, quality are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1717\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to results/checkpoint-11589\n",
            "Configuration saved in results/checkpoint-11589/config.json\n",
            "Model weights saved in results/checkpoint-11589/pytorch_model.bin\n",
            "tokenizer config file saved in results/checkpoint-11589/tokenizer_config.json\n",
            "Special tokens file saved in results/checkpoint-11589/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=11589, training_loss=0.2734483408962891, metrics={'train_runtime': 6152.8839, 'train_samples_per_second': 7.534, 'train_steps_per_second': 1.884, 'total_flos': 3518030302483200.0, 'train_loss': 0.2734483408962891, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_example = ds['test'][7]\n",
        "print(test_example['text'])\n",
        "print(test_example['drs'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h4QG_fr0m9w",
        "outputId": "6ff9ef85-21e8-486b-bf32-b2c695786561"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My brother wants to kill me.\n",
            "person.n.01 Role +1\n",
            "brother.n.01 Of speaker\n",
            "want.v.01 Pivot -2 Time +1 Theme +2\n",
            "time.n.08 EQU now\n",
            "kill.v.01 Agent -4 Patient +1\n",
            "person.n.01 EQU speaker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    torch.tensor([test_example['input_ids']]).cuda(), \n",
        "    max_new_tokens=512,\n",
        "    num_beams=3\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "_WRCsAwb5iha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278439ab-0161-4509-ce58-0f244b802d94"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "person.n.01 Role +1\n",
            "brother.n.01 Of speaker\n",
            "want.v.01 Pivot -2 Time +1 Theme +2\n",
            "time.n.08 EQU now\n",
            "kill.v.01 Agent -3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(ds['test'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "eyd2VRCwkX-1",
        "outputId": "ae05fa86-e739-47b4-b161-c7272f1ee71a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: lang, text, drs, quality. If lang, text, drs, quality are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1717\n",
            "  Batch size = 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "decoded_preds = tokenizer.batch_decode(preds[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "yqxgv97cpqVb"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for y_true_drg, y_pred_drg in zip(ds['test'], decoded_preds):\n",
        "  try:\n",
        "    y_true_penman = SBNGraph().from_string(y_true_drg['drs']).to_penman_string()\n",
        "  except Exception as e:\n",
        "    print('error in GS', [y_true_drg], e, '', sep='\\n')\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    y_pred_penman = SBNGraph().from_string(y_pred_drg).to_penman_string()\n",
        "  except Exception as e:\n",
        "    # print('error in pred', [y_pred_drg], e, '', sep='\\n')\n",
        "    continue\n",
        "  \n",
        "  y_true.append(y_true_penman)\n",
        "  y_pred.append(y_pred_penman)"
      ],
      "metadata": {
        "id": "ExeC0-bdqsC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_scores = defaultdict(list)\n",
        "for yt, yp in zip(y_true, y_pred):\n",
        "  with open('tempgold', \"w\") as gold_f:\n",
        "    gold_f.write(yt)\n",
        "\n",
        "  with open('temppred', \"w\") as pred_f:\n",
        "      pred_f.write(yp)\n",
        "\n",
        "  scores = smatch_score('/content/tempgold', '/content/temppred')\n",
        "  for k, v in scores.items():\n",
        "    total_scores[k].append(v)"
      ],
      "metadata": {
        "id": "vWfe6pebp3CD"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_scores = {k: sum(v) / len(v) for k, v in total_scores.items()}\n",
        "final_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTII6Z_u2Isz",
        "outputId": "4163e311-1fd8-4db0-aa31-420eb46f5cd7"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision': 0.897610410918853,\n",
              " 'recall': 0.8831095488976565,\n",
              " 'f1': 0.8860603832265207}"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    }
  ]
}